README
Matt Dorsett

CLASSES/FILES:

Defines:

NumberClassification.h

Utility classes:

SegmentMask: SegmentMask.h
PixelStats: PixelStats.cpp PixelStats.h

Abstract classes:

NumberClassifier: NumberClassifier.cpp NumberClassifier.h
MLClassifier: MLClassifier.cpp MLClassifier.h

Classifiers:

NaiveBayes: NaiveBayes.cpp NaiveBayes.h
NonMLClassifier (deprecated): NonMLClassifier.cpp NonMLClassifier.h

Test code:

driver: driver.cpp driver.h

INHERITANCE:

NumberClassifier > MLClassifier > NaiveBayes
NumberClassifier > NonMLClassifier

HARDCODED BEHAVIOR (not easily modifiable):

The code creates two different masks, one for horizontal segments and
one for vertical segments. You supply it with a function that it uses to compute
the vertical segment mask, and it will use that same function to compute the
horizontal mask, swapping the axes. Thus every mask is the same size and shape, but the
orientation can very by 90 degrees. The masks are used to split the image into seven 
different segments.

The positioning of the dots is hardcoded according to the model used by the current
version of the form generator. This model is as follows:

Horizontal profile:
  LEFT <--1/4--> DOT <--1/2--> DOT <--1/4--> RIGHT
Vertical profile:
  TOP <--1/6--> DOT <--1/3--> DOT <--1/3--> DOT <--1/6--> BOTTOM

These profiles are combined to create three rows of two dots. If this is changed, then
there is no easy way to modify the code to comply; NumberClassifer::find_rois() will need
to be modified.

CREATING MASK FUNCTIONS

Each mask is bounded by a rectangle, the size of which is defined by the parameters passed
into NumberClassifier::NumberClassifier(). The mask function will take two parameters,
x and y, and it should be defined on the domain {[0,width),[0,height)}. If the point
(x,y) should be included in the mask, then the f(x,y) should output 1, otherwise it
should output 0. See the examples for rectangular, elliptical, and diamond masks.

CREATING A NEW ML CLASSIFIER

Create a class that extends MLClassifier, and you will need to define at least two
functions, c_process() and t_process(). These are the functions that train on and
classify individual images. You may need to include additional state that keeps track
of the training data. See the PixelStats member in NaiveBayes to see an example of
how to store the mean and variance of each segment's pixel counts.

NOTES ABOUT THE CURRENT CROP/ALIGNMENT ALGORITHM

Currently, the function NumberClassifier::crop_img() takes advantage of the fact that the
six dots are always in the same positions relative to each other, so it searches for the darkest
region of 2x2 rectangles that are spaced out in the same pattern as the dots. It then centers
a rectangle of the desired image size (in the standard case 60x40) around those dots. In the 
not-so-rare case that a sliver of the image is cut out of the original image, then the crop
function crops it to be smaller than needed, and it is then stretched to the desired size
by a subsequent call to cv::resize().

NOTES ABOUT NAIVE BAYES IMPLEMENTATION

Naive Bayes works by measuring a bunch of features, assuming the features are independent 
events, and calculating the probability that a given example belongs to a class by 
multiplying the probabilities of each feature given the measured probabilities of the
features in the class. For example, when trying to train a "0" model, we take a bunch
of handwritten zeros, measure the total mean and variance of the number of black pixels
in each segment of those test examples, and end up with seven different normal distributions.
We replicate this for the numbers 1-9. If we want to use the training data to classify an
image, we need to calculate the probability that each segment came from each of the ten
possible numbers. We multiply the probabilities that the segments came from a 0, then the
probabilities that the segments came from a 1, etc. These probability calculations are
simply done by calculating f(x, mu, s), where x is the number of black pixels in a given 
segment, mu is the measured mean number of black pixels in that segment of that number, s 
is the standard deviation, and f is the probability density function for the normal
distribution. Once all seven probailities are computed for a given number, we multiply them
together (we assume that each is independent of the others, that is the magic of Naive
Bayes). After doing this for all ten numbers, we have ten different probailities we can
compare, and we choose number that yielded the largest probability.
